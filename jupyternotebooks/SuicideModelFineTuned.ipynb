{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "mount_file_id": "1c8zQv1N9MN7D3rJIf-u2RJ68-kKS0sKW",
      "authorship_tag": "ABX9TyOotqFz+yXwiI6IH+HapqG4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArunKoundinya/SoulGuard/blob/master/jupyternotebooks/SuicideModelFineTuned.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_J7G2SDzT_kz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39d94eb6-b445-42fd-eef5-92f0c4d0a2bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Embedding, Dropout, Bidirectional, LSTM, Dense, Flatten\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers import Adam,RMSprop\n",
        "from tensorflow.keras.initializers import GlorotUniform\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"/content/drive/MyDrive/DeepLearning/Capstone-SoulGuard\")"
      ],
      "metadata": {
        "id": "3T0K7anfxcE4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SuicideDetection = pd.read_csv('https://media.githubusercontent.com/media/ArunKoundinya/SoulGuard/refs/heads/master/data/SoulG_Update.csv')\n",
        "X = SuicideDetection['cleaned_text']\n",
        "X = X.astype(str)\n",
        "y = SuicideDetection['class']\n",
        "y = y.astype(str)\n",
        "y = pd.factorize(y)[0]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "tokenizer = Tokenizer(oov_token=\"<UNK>\",)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "tokenizer.word_index['<PAD>'] = 0\n",
        "\n",
        "X_sequences_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_sequences_test = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "X_train_padded = pad_sequences(X_sequences_train, padding='post', maxlen=100)\n",
        "X_test_padded = pad_sequences(X_sequences_test, padding='post', maxlen=100)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index)"
      ],
      "metadata": {
        "id": "KBJbekEgxqXQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_embeddings(glove_path):\n",
        "    embedding_index = {}\n",
        "    with open(glove_path, encoding=\"utf8\") as glove_file:\n",
        "        for line in glove_file:\n",
        "            word, coefs = line.split(maxsplit=1)\n",
        "            coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "            embedding_index[word] = coefs\n",
        "    return embedding_index\n",
        "\n",
        "def create_embedding_matrix(embedding_index, word2idx, vocab_size, embedding_dim):\n",
        "    mat=np.zeros((vocab_size,embedding_dim))\n",
        "    for key,value in word2idx.items():\n",
        "      mat[value]=embedding_index.get(key)\n",
        "    mat[np.isnan(mat)] = 0\n",
        "    return mat\n",
        "\n",
        "glove_path = f\"/content/drive/My Drive/MSIS/IntroductiontoDeepLearning/Project/glove.6B/glove.twitter.27B.200d.txt\"\n",
        "embedding_index = load_embeddings(glove_path)\n",
        "\n",
        "word2idx = tokenizer.word_index\n",
        "embedding_dim = 200\n",
        "embedding_matrix = create_embedding_matrix(embedding_index, word2idx, vocab_size, embedding_dim)"
      ],
      "metadata": {
        "id": "lSwgCkOdxzTS"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "with open('best_hyperparameters.json', 'r') as f:\n",
        "    best_hyperparameters = json.load(f)\n",
        "\n",
        "best_hyperparameters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R49VBg_CyJUT",
        "outputId": "74f1b066-9517-4a23-cb43-39c0f1d3aa0e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'lstm_units_1': 32,\n",
              " 'lstm_units_2': 64,\n",
              " 'lstm_units_3': 28,\n",
              " 'dense_units_1': 8,\n",
              " 'dense_units_2': 8,\n",
              " 'dropout_rate': 0.05,\n",
              " 'learning_rate': 0.0022173357103291887,\n",
              " 'tuner/epochs': 4,\n",
              " 'tuner/initial_epoch': 2,\n",
              " 'tuner/bracket': 2,\n",
              " 'tuner/round': 1,\n",
              " 'tuner/trial_id': '0037'}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "\n",
        "# Extract the best hyperparameters from the dictionary\n",
        "lstm_units_1 = best_hyperparameters['lstm_units_1']\n",
        "lstm_units_2 = best_hyperparameters['lstm_units_2']\n",
        "lstm_units_3 = best_hyperparameters['lstm_units_3']\n",
        "dense_units_1 = best_hyperparameters['dense_units_1']\n",
        "dense_units_2 = best_hyperparameters['dense_units_2']\n",
        "dropout_rate = best_hyperparameters['dropout_rate']\n",
        "learning_rate = best_hyperparameters['learning_rate']\n",
        "\n",
        "\n",
        "def build_model_with_l2(lstm_units_1, lstm_units_2, lstm_units_3, dense_units_1, dense_units_2, dropout_rate, learning_rate):\n",
        "    inputs = Input(shape=(100,))\n",
        "\n",
        "    embedding_layer = Embedding(input_dim=vocab_size, output_dim=200, input_length=100, weights=[embedding_matrix], trainable=False)(inputs)\n",
        "\n",
        "    bilstm = Bidirectional(LSTM(lstm_units_1, activation='tanh', return_sequences=True, kernel_regularizer=l2(0.001)))(embedding_layer)\n",
        "    bilstm = BatchNormalization()(bilstm)\n",
        "    bilstm = Bidirectional(LSTM(lstm_units_2, activation='tanh', return_sequences=True, kernel_regularizer=l2(0.001)))(bilstm)\n",
        "    bilstm = BatchNormalization()(bilstm)\n",
        "    bilstm = Bidirectional(LSTM(lstm_units_3, activation='tanh', return_sequences=True, kernel_regularizer=l2(0.001)))(bilstm)\n",
        "    bilstm = BatchNormalization()(bilstm)\n",
        "\n",
        "    bilstm = Dropout(dropout_rate)(bilstm)\n",
        "\n",
        "    flatten = Flatten()(bilstm)\n",
        "    dense = Dense(dense_units_1, activation=\"relu\", kernel_regularizer=l2(0.001))(flatten)\n",
        "    dense = Dense(dense_units_2, activation=\"relu\", kernel_regularizer=l2(0.001))(dense)\n",
        "    outputs = Dense(1, activation='sigmoid')(dense)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    # Define exponential decay schedule\n",
        "    lr_schedule = ExponentialDecay(\n",
        "    initial_learning_rate=learning_rate,  # Best hyperparameter value\n",
        "    decay_steps=1000,  # How often to decay the learning rate\n",
        "    decay_rate=0.9,    # Factor to decay the learning rate\n",
        "    staircase=True     # If True, the learning rate decreases in steps\n",
        "    )\n",
        "\n",
        "    # Use the schedule in your optimizer\n",
        "    optimizer = Adam(learning_rate=lr_schedule)\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create the model with the best hyperparameters\n",
        "best_model = build_model_with_l2(lstm_units_1, lstm_units_2, lstm_units_3, dense_units_1, dense_units_2, dropout_rate, learning_rate)\n",
        "\n",
        "# Train the model if necessary\n",
        "best_model.fit(X_train_padded, y_train, batch_size = 64, epochs=20, validation_data=(X_test_padded, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d61y8tc-yJuM",
        "outputId": "3b8e4a16-592e-4b72-f9af-3b2779bd3945"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m2698/2698\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 35ms/step - accuracy: 0.8876 - loss: 0.5079 - val_accuracy: 0.9182 - val_loss: 0.2552\n",
            "Epoch 2/20\n",
            "\u001b[1m2698/2698\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 35ms/step - accuracy: 0.9225 - loss: 0.2487 - val_accuracy: 0.9185 - val_loss: 0.2522\n",
            "Epoch 3/20\n",
            "\u001b[1m2698/2698\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 35ms/step - accuracy: 0.9264 - loss: 0.2263 - val_accuracy: 0.9227 - val_loss: 0.2276\n",
            "Epoch 4/20\n",
            "\u001b[1m2698/2698\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 34ms/step - accuracy: 0.9312 - loss: 0.2081 - val_accuracy: 0.9233 - val_loss: 0.2209\n",
            "Epoch 5/20\n",
            "\u001b[1m2698/2698\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 35ms/step - accuracy: 0.9327 - loss: 0.2006 - val_accuracy: 0.9316 - val_loss: 0.1983\n",
            "Epoch 6/20\n",
            "\u001b[1m2698/2698\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 35ms/step - accuracy: 0.9357 - loss: 0.1896 - val_accuracy: 0.9313 - val_loss: 0.1980\n",
            "Epoch 7/20\n",
            "\u001b[1m2698/2698\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 35ms/step - accuracy: 0.9377 - loss: 0.1841 - val_accuracy: 0.9320 - val_loss: 0.1955\n",
            "Epoch 8/20\n",
            "\u001b[1m2698/2698\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 35ms/step - accuracy: 0.9408 - loss: 0.1761 - val_accuracy: 0.9342 - val_loss: 0.1931\n",
            "Epoch 9/20\n",
            "\u001b[1m2698/2698\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 35ms/step - accuracy: 0.9414 - loss: 0.1733 - val_accuracy: 0.9333 - val_loss: 0.1918\n",
            "Epoch 10/20\n",
            "\u001b[1m2698/2698\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 35ms/step - accuracy: 0.9424 - loss: 0.1696 - val_accuracy: 0.9332 - val_loss: 0.1877\n",
            "Epoch 11/20\n",
            "\u001b[1m2698/2698\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 35ms/step - accuracy: 0.9434 - loss: 0.1671 - val_accuracy: 0.9353 - val_loss: 0.1877\n",
            "Epoch 12/20\n",
            "\u001b[1m2698/2698\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 35ms/step - accuracy: 0.9441 - loss: 0.1646 - val_accuracy: 0.9340 - val_loss: 0.1851\n",
            "Epoch 13/20\n",
            "\u001b[1m2698/2698\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 35ms/step - accuracy: 0.9453 - loss: 0.1618 - val_accuracy: 0.9350 - val_loss: 0.1858\n",
            "Epoch 14/20\n",
            "\u001b[1m2698/2698\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 35ms/step - accuracy: 0.9460 - loss: 0.1597 - val_accuracy: 0.9348 - val_loss: 0.1860\n",
            "Epoch 15/20\n",
            "\u001b[1m2698/2698\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 35ms/step - accuracy: 0.9476 - loss: 0.1565 - val_accuracy: 0.9353 - val_loss: 0.1858\n",
            "Epoch 16/20\n",
            "\u001b[1m2698/2698\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 35ms/step - accuracy: 0.9476 - loss: 0.1555 - val_accuracy: 0.9345 - val_loss: 0.1862\n",
            "Epoch 17/20\n",
            "\u001b[1m2698/2698\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 35ms/step - accuracy: 0.9479 - loss: 0.1557 - val_accuracy: 0.9353 - val_loss: 0.1860\n",
            "Epoch 18/20\n",
            "\u001b[1m2698/2698\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 35ms/step - accuracy: 0.9474 - loss: 0.1570 - val_accuracy: 0.9350 - val_loss: 0.1872\n",
            "Epoch 19/20\n",
            "\u001b[1m2698/2698\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 35ms/step - accuracy: 0.9481 - loss: 0.1550 - val_accuracy: 0.9352 - val_loss: 0.1861\n",
            "Epoch 20/20\n",
            "\u001b[1m2698/2698\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 35ms/step - accuracy: 0.9497 - loss: 0.1518 - val_accuracy: 0.9350 - val_loss: 0.1863\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7f7520276200>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('suicide_detection_model.pkl', 'wb') as f:\n",
        "    pickle.dump(best_model, f)"
      ],
      "metadata": {
        "id": "m31_2855y1Ls"
      },
      "execution_count": 15,
      "outputs": []
    }
  ]
}