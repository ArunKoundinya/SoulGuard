{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArunKoundinya/SoulGuard/blob/master/jupyternotebooks/SuicideClassification_Sentiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nebuMxlen-xn"
      },
      "source": [
        "This notebook consists of both Suicide Classification & Sentiment Models. Althought these models have been developed independently, these integrated here for purpose of future CI/CD."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EIOg44up115"
      },
      "source": [
        "This notebook have following sections:\n",
        "1.   Loading Libraries\n",
        "2.   Data Loading\n",
        "3.   Loading Glove Embeddings\n",
        "4.   Suicide Classification Model\n",
        "5.   Pickling the Suicide Classification Model\n",
        "6.   Data Cleaning for Sentiment Model\n",
        "7.   Custom Made TextBlob, Vader & WorryWords\n",
        "8.   Developing a sample py file for testing purpose for integration\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-cZk3Ds0TQV"
      },
      "source": [
        "## Loading Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZJAVP1V0lib",
        "outputId": "cdd2d192-f348-4379-9920-73331f5020c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting vaderSentiment\n",
            "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl.metadata (572 bytes)\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.14.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from vaderSentiment) (2.32.3)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob) (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (4.66.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2024.8.30)\n",
            "Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading emoji-2.14.0-py3-none-any.whl (586 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m586.9/586.9 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji, vaderSentiment\n",
            "Successfully installed emoji-2.14.0 vaderSentiment-3.3.2\n"
          ]
        }
      ],
      "source": [
        "pip install vaderSentiment emoji textblob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glPkodTqkPS8",
        "outputId": "9c90278f-1c61-4fca-cfcc-6b5cbd8047de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Embedding, Dropout, Bidirectional, LSTM, Dense, Flatten\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers import Adam,RMSprop\n",
        "from tensorflow.keras.initializers import GlorotUniform\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "\n",
        "from textblob import TextBlob\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from textblob import TextBlob\n",
        "from collections import Counter\n",
        "import emoji  # Make sure to import the emoji module\n",
        "\n",
        "# Download VADER lexicon if not already done\n",
        "nltk.download('vader_lexicon')\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TrL6g1ec8g2T"
      },
      "outputs": [],
      "source": [
        "SuicideDetection = pd.read_csv('https://media.githubusercontent.com/media/ArunKoundinya/SoulGuard/refs/heads/master/data/SoulG_Update.csv')\n",
        "SuicideDetection = SuicideDetection.sample(n=200000, random_state=42)\n",
        "X = SuicideDetection['cleaned_text']\n",
        "X = X.astype(str)\n",
        "y = SuicideDetection['class']\n",
        "y = y.astype(str)\n",
        "y = pd.factorize(y)[0]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "tokenizer = Tokenizer(oov_token=\"<UNK>\",)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "tokenizer.word_index['<PAD>'] = 0\n",
        "\n",
        "X_sequences_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_sequences_test = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "X_train_padded = pad_sequences(X_sequences_train, padding='post', maxlen=100)\n",
        "X_test_padded = pad_sequences(X_sequences_test, padding='post', maxlen=100)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxLDtZOh80Dx",
        "outputId": "58579aee-6d63-422e-c0cc-0707bd544a6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "def load_embeddings(glove_path):\n",
        "    embedding_index = {}\n",
        "    with open(glove_path, encoding=\"utf8\") as glove_file:\n",
        "        for line in glove_file:\n",
        "            word, coefs = line.split(maxsplit=1)\n",
        "            coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "            embedding_index[word] = coefs\n",
        "    return embedding_index\n",
        "\n",
        "def create_embedding_matrix(embedding_index, word2idx, vocab_size, embedding_dim):\n",
        "    mat=np.zeros((vocab_size,embedding_dim))\n",
        "    for key,value in word2idx.items():\n",
        "      mat[value]=embedding_index.get(key)\n",
        "    mat[np.isnan(mat)] = 0\n",
        "    return mat\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "glove_path = f\"/content/drive/My Drive/MSIS/IntroductiontoDeepLearning/Project/glove.6B/glove.twitter.27B.200d.txt\"\n",
        "embedding_index = load_embeddings(glove_path)\n",
        "\n",
        "word2idx = tokenizer.word_index\n",
        "embedding_dim = 200\n",
        "embedding_matrix = create_embedding_matrix(embedding_index, word2idx, vocab_size, embedding_dim)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/DeepLearning/Capstone-SoulGuard/vocab_dict.pkl', 'wb') as f:\n",
        "  pickle.dump(tokenizer.word_index, f)"
      ],
      "metadata": {
        "id": "3M3JGl7jXrxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MAuPlCJ818U",
        "outputId": "c0b60ee4-b69d-43ff-a397-ae73a3750a40"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 31ms/step - accuracy: 0.8868 - loss: 0.2764 - val_accuracy: 0.9281 - val_loss: 0.1872\n",
            "Epoch 2/10\n",
            "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 31ms/step - accuracy: 0.9343 - loss: 0.1708 - val_accuracy: 0.9347 - val_loss: 0.1684\n",
            "Epoch 3/10\n",
            "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 31ms/step - accuracy: 0.9399 - loss: 0.1560 - val_accuracy: 0.9360 - val_loss: 0.1672\n",
            "Epoch 4/10\n",
            "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 31ms/step - accuracy: 0.9448 - loss: 0.1455 - val_accuracy: 0.9367 - val_loss: 0.1647\n",
            "Epoch 5/10\n",
            "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 31ms/step - accuracy: 0.9487 - loss: 0.1347 - val_accuracy: 0.9343 - val_loss: 0.1748\n",
            "Epoch 6/10\n",
            "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 31ms/step - accuracy: 0.9519 - loss: 0.1256 - val_accuracy: 0.9353 - val_loss: 0.1745\n",
            "Epoch 7/10\n",
            "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 31ms/step - accuracy: 0.9559 - loss: 0.1186 - val_accuracy: 0.9357 - val_loss: 0.1755\n",
            "Epoch 8/10\n",
            "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 30ms/step - accuracy: 0.9583 - loss: 0.1107 - val_accuracy: 0.9356 - val_loss: 0.1805\n",
            "Epoch 9/10\n",
            "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 30ms/step - accuracy: 0.9614 - loss: 0.1037 - val_accuracy: 0.9337 - val_loss: 0.1873\n",
            "Epoch 10/10\n",
            "\u001b[1m5000/5000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 31ms/step - accuracy: 0.9622 - loss: 0.1009 - val_accuracy: 0.9329 - val_loss: 0.1904\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7ac834576ce0>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# Input layer\n",
        "inputs = Input(shape=(100,))\n",
        "\n",
        "embedding_layer = Embedding(input_dim=vocab_size, output_dim=200, input_length=100, weights=[embedding_matrix], trainable=False)(inputs)\n",
        "bilstm = Bidirectional(LSTM(16, activation='tanh', return_sequences=True))(embedding_layer)\n",
        "bilstm = Bidirectional(LSTM(8, activation='tanh', return_sequences=True))(bilstm)\n",
        "bilstm = Bidirectional(LSTM(4, activation='tanh', return_sequences=True))(bilstm)\n",
        "flatten = Flatten()(bilstm)\n",
        "dense = Dense(16, activation=\"relu\")(flatten)\n",
        "dense = Dense(4, activation=\"relu\")(dense)\n",
        "outputs = Dense(1, activation='sigmoid')(dense)\n",
        "\n",
        "# Build the model\n",
        "model_lstm_bi_embed = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "model_lstm_bi_embed.build(input_shape=(None, 100))  # Batch size unspecified\n",
        "\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model_lstm_bi_embed.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model_lstm_bi_embed.fit(X_train_padded, y_train, epochs=10, validation_data=(X_test_padded, y_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('suicide_detection_model.pkl', 'wb') as f:\n",
        "    pickle.dump(model_lstm_bi_embed, f)"
      ],
      "metadata": {
        "id": "_aE-TJIKofsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentiment Model\n"
      ],
      "metadata": {
        "id": "MhuINv79wzcO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define stopwords and lemmatizer\n",
        "custom_stopwords = {'knew', 'become', 'way', 'put', 'took', 'im', 'etc', 'went', 'got', 'yet',\n",
        "                    'literally', 'na', 'even', 'gon', 'id', 'wan', 'due', 'instead', 've',\n",
        "                    't', 'hes', 'ket', 'lot', 'ask', 'many', 'u', 'ni', 'cum', 'basically',\n",
        "                    'cecil', 'tell', 'stuff', 'use', 'put', 'seem', 'yet', 'yeah', 'done', 'im',\n",
        "                    'least', 'eve', 'let', 'may', 'actually', 'lol', 'cake', 'give',\n",
        "                    'ta', 'na', 'give', 'got', 'something', 'like', 'ive', 'ye', 'filler', 'fillerfiller','ampx200b','gtpoplt',\n",
        "                    'pog', 'penis', 'bacon', 'bruh', 'corn', 'title', 'discochocolate', 'fuck', 'sus', 'gtbyelt', 'as', 'gt', 'lt',\n",
        "                    'pop', 'amp', 'ampx200b', 'gt', 'jake', 'paul', 'cheese', 'x200b','ur','1','cum', 'brazil'}\n",
        "\n",
        "stop_en = set(stopwords.words('english')).union(custom_stopwords)\n",
        "\n",
        "stop_words = set(stop_en) - { 'not', 'no', 'couldn', \"couldn't\", \"wouldn't\", \"shouldn't\", \"isn't\",\n",
        "                                                \"aren't\", \"wasn't\", \"weren't\", \"don't\", \"doesn't\", \"hadn't\", \"hasn't\",\n",
        "                                                 \"won't\", \"can't\", \"mightn't\",\"needn't\",\"nor\",\"shouldn\",\"should've\",\"should\",\n",
        "                                                 \"weren\",\"wouldn\",\"mustn't\",\"mustn\",\"didn't\",\"didn\",\"doesn\",\"did\",\"does\",\"hadn\",\n",
        "                                                 \"hasn\",\"haven't\",\"haven\",\"needn\",\"shan't\"}\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Function to preprocess the text\n",
        "def custom_preprocess(text):\n",
        "    # 1. Remove URLs and replace them with a space\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', ' ', text)\n",
        "\n",
        "    # 2. Replace specific special characters and punctuation with spaces\n",
        "    text = re.sub(r'[!@#$%^&*()\\-={}[\\]\\\\|;:\"\\'<>,.?/`~]+', ' ', text)\n",
        "\n",
        "    # 3. Remove long binary numbers and meaningless long repetitive characters\n",
        "    text = re.sub(r'\\b[01]{10,}\\b', ' ', text)  # Long binary numbers\n",
        "    text = re.sub(r'(.)\\1{5,}', ' ', text)  # Characters repeated more than 5 times\n",
        "\n",
        "    # 4. Tokenize the text\n",
        "    tokens = word_tokenize(text.lower())\n",
        "\n",
        "    # 5. Remove tokens longer than 25 characters\n",
        "    tokens = [token for token in tokens if len(token) < 26]\n",
        "\n",
        "    # 6. Remove stopwords\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    # 7. Lemmatize the tokens\n",
        "    tokens = [lemmatizer.lemmatize(token, pos='v') for token in tokens]\n",
        "\n",
        "    # Join tokens back to a single string\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Apply the preprocessing function to the dataset\n",
        "SuicideDetection['cleaned_text1'] = SuicideDetection['text'].apply(custom_preprocess)\n"
      ],
      "metadata": {
        "id": "kZMw1njwNzkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# custom worrywords\n",
        "Worry= pd.read_csv(\"/content/drive/MyDrive/DeepLearning/Capstone-SoulGuard/worrywords-v1.csv\")\n",
        "Worry = Worry[Worry['Mean']>0]\n",
        "worrywords_dict = dict(zip(Worry['Term'],Worry['Mean']))\n",
        "\n",
        "# custom words\n",
        "custom_lexicon = {\n",
        "    'suicide': -1.0, 'depression': -1.0, 'hurt': -0.8, 'pain': -0.8, 'loneliness': -0.8, 'struggle': -0.5, 'failure': -0.6, 'hope': 0.8, 'help': 0.7,\n",
        "    'love': 0.9, 'support': 0.8, 'peace': 0.6, 'family': 0.7, 'friend': 0.8, 'happy': 1.0, 'life': 0.5, 'future': 0.5, 'escape': -0.4, 'numb': -0.6,\n",
        "    'scared': -0.5, 'broken': -0.8, 'lost': -0.7, 'anxious': -0.5,\n",
        "    'kill': -0.9, 'stop': -0.4, 'abuse': -0.9, 'guilty': -0.6, 'commit': -0.5, 'fake': -0.5, 'dead': -0.8, 'stress': -0.6, 'depress': -0.9, 'fail': -0.7,\n",
        "    'death': -1.0, 'lose': -0.5, 'fear': -0.6, 'scar': -0.4, 'angry': -0.7, 'trauma': -0.8, 'cruel': -0.8, 'poison': -0.8, 'unlovable': -0.9,\n",
        "    'lonely': -0.8, 'mistake': -0.5, 'destroy': -0.8, 'miserable': -0.9, 'mess': -0.4, 'die': -1.0, 'cry': -0.6, 'tear': -0.5, 'guilt': -0.6,\n",
        "    'threat': -0.7, 'hopeless': -1.0, 'despair': -0.9, 'misery': -0.9, 'sorrow': -0.8, 'grief': -0.8, 'worthless': -0.9, 'anxiety': -0.7, 'upset': -0.5,\n",
        "    'panic': -0.6, 'rage': -0.8, 'distress': -0.7, 'shattered': -0.9, 'inadequate': -0.7, 'rejected': -0.8, 'unloved': -0.9, 'cursed': -0.8,\n",
        "    'burdened': -0.8, 'restless': -0.4, 'toxic': -0.8, 'suffer': -0.8, 'isolate': -0.7, 'discourage': -0.5, 'frighten': -0.6, 'struggling': -0.7,\n",
        "    'manipulate': -0.5, 'cheat': -0.5, 'waste': -0.6, 'resent': -0.5, 'regret': -0.6, 'grudge': -0.6, 'detest': -0.7, 'void': -0.8, 'wreck': -0.7,\n",
        "    'mourn': -0.8\n",
        "}\n",
        "\n",
        "#  custom emoji lexicon\n",
        "emoji_lexicon = {'😂': 0.7, '😔': -0.5, '😏': 0.2, '😝': 0.6, '😘': 0.8, '❤': 0.9, '😳': 0.0, '😎': 0.6, '🥴': -0.3, '🙄': -0.1, '😭': -0.9,\n",
        "                 '😬': -0.4, '🤭': 0.3, '😩': -0.6, '🤔': 0.0, '🥰': 0.9, '😀': 1.0, '🤗': 0.8, '😡': -0.8, '🤧': -0.6, '😐': 0.0,\n",
        "                 '😁': 0.6, '😊': 0.7, '♥': 0.9, '😠': -0.7, '🥵': -0.5, '💜': 0.9, '💙': 0.8, '😈': -0.3, '💃': 0.5, '😍': 1.0,\n",
        "                 '💕': 0.9, '🤯': -0.2, '🥳': 0.9, '😻': 1.0, '😤': -0.4, '🤣': 0.8, '😥': -0.7, '😖': -0.7, '🙂': 0.4, '😞': -0.8,\n",
        "                 '😓': -0.6, '😪': -0.6}\n",
        "\n",
        "def extract_emojis(text):\n",
        "    return [char for char in text if emoji.is_emoji(char)]"
      ],
      "metadata": {
        "id": "VWsthvf-fV5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for calculating score for custom words\n",
        "def calculation_custom_score(filter, customdict , text):\n",
        "      words = word_tokenize(text.lower())\n",
        "      scores = []\n",
        "\n",
        "      for word in words:\n",
        "        if word in customdict:\n",
        "          scores.append(customdict[word])\n",
        "\n",
        "      if scores:\n",
        "        score = sum(scores) / len(scores)\n",
        "        if filter == \"worry\":\n",
        "          score = score / 3\n",
        "          score = score * -1\n",
        "        return score\n",
        "      else :\n",
        "        return 0"
      ],
      "metadata": {
        "id": "JUUlcJjLR3p5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize VADER Sentiment Analyzer\n",
        "vader_analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Function to get TextBlob and VADER sentiment scores\n",
        "def hybrid_sentiment_analysis_worry(text):\n",
        "    # TextBlob sentiment analysis\n",
        "    blob = TextBlob(text)\n",
        "    textblob_polarity = blob.sentiment.polarity  # Range from -1 (negative) to +1 (positive)\n",
        "\n",
        "    # VADER sentiment analysis\n",
        "    vader_scores = vader_analyzer.polarity_scores(text)\n",
        "    vader_compound = vader_scores['compound']  # Range from -1 to +1\n",
        "\n",
        "    # Custom lexicon scoring\n",
        "    custom_score = calculation_custom_score(\"custom\",custom_lexicon,text)\n",
        "\n",
        "    # worrywords\n",
        "    worry_score = calculation_custom_score(\"worry\",worrywords_dict,text)\n",
        "\n",
        "    # Emoji scoring\n",
        "    emoji_score = calculation_custom_score(\"emoji\",emoji_lexicon,text)\n",
        "\n",
        "    # Combine scores with adjustable weights\n",
        "    combined_score = (textblob_polarity * 0.10 + vader_compound * 0.50 +\n",
        "                      custom_score * 0.10 + emoji_score * 0.10 + worry_score*0.20)\n",
        "\n",
        "    # Scale the combined score to a 0 to 1 range\n",
        "    scaled_score = (combined_score + 1) / 2\n",
        "\n",
        "    return scaled_score"
      ],
      "metadata": {
        "id": "fvqDQcObNoG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SuicideDetection['text'][100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "1FYwQJoYV4e3",
        "outputId": "d76de4f4-71e3-4b7a-bc61-69a206ee0c10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Check out this new horror short I made https://youtu.be/S50McngM1ws it took me a while to make so hope you enjoy it'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing"
      ],
      "metadata": {
        "id": "FTqHGMasV8O6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_bistm_pretrained = pickle.load(open(\"/content/drive/MyDrive/DeepLearning/Capstone-SoulGuard/suicide_detection_model.pkl\", 'rb'))\n",
        "vocab_dict = pickle.load(open('/content/drive/MyDrive/DeepLearning/Capstone-SoulGuard/vocab_dict.pkl', 'rb'))\n"
      ],
      "metadata": {
        "id": "Z7eQe6vHV40H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import warnings\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def preprocess(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    # Tokenize text into words\n",
        "    words = word_tokenize(text)\n",
        "    # Remove stopwords\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    # Lemmatize words\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    # Join the words back into a single string\n",
        "    text = ' '.join(words)\n",
        "    return text\n",
        "\n",
        "stop_words = set(stopwords.words('english')) - { 'not', 'no', 'couldn', \"couldn't\", \"wouldn't\", \"shouldn't\", \"isn't\",\n",
        "                                                \"aren't\", \"wasn't\", \"weren't\", \"don't\", \"doesn't\", \"hadn't\", \"hasn't\",\n",
        "                                                 \"won't\", \"can't\", \"mightn't\",\"needn't\",\"nor\",\"shouldn\",\"should've\",\"should\",\n",
        "                                                 \"weren\",\"wouldn\",\"mustn't\",\"mustn\",\"didn't\",\"didn\",\"doesn\",\"did\",\"does\",\"hadn\",\n",
        "                                                 \"hasn\",\"haven't\",\"haven\",\"needn\",\"shan't\"}\n",
        "\n",
        "def process_sentence(sentence):\n",
        "  list1 = []\n",
        "  for word in sentence.split():\n",
        "    if word in vocab_dict:\n",
        "      list1.append(vocab_dict[word])\n",
        "    else:\n",
        "      list1.append(vocab_dict[\"<UNK>\"])\n",
        "  return list1\n",
        "\n",
        "def format_examples(data1, vocab_dict, maxlen):\n",
        "  sequences_data=data1['cleaned_text'].apply(process_sentence).tolist()\n",
        "  padded_sequences_data = pad_sequences(sequences_data,padding='post', maxlen=100)\n",
        "  return padded_sequences_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvYIvVsrW9Rg",
        "outputId": "9423cb1f-135f-4a1e-dbef-92967d365e1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = SuicideDetection['text'][100]\n",
        "\n",
        "cleanedtext = preprocess(text)\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'text' : [text],\n",
        "    'cleaned_text': [cleanedtext]\n",
        "})\n",
        "\n",
        "X_input = format_examples(df, vocab_dict, 100)\n",
        "prediction = model_bistm_pretrained.predict(X_input).astype(float)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGYOfKKuZA8J",
        "outputId": "1b2a15f5-65b0-404e-adeb-9660f3bd15ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prediction.max()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOy2TsHBap1c",
        "outputId": "c2e4c04a-6be5-4878-f365-68983dcc63ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9699795842170715"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hybrid_sentiment_analysis_worry(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11TWIbnzax4o",
        "outputId": "f00e3672-bae7-47c8-df54-f5b3eea80969"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6605810606060606"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WGDL5B74bCbD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "1iew6tdRhNcvjbRhL0ovCwg20FnRQ-di9",
      "authorship_tag": "ABX9TyOmvt/4DEMPL0NTMk9f/1sM",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}